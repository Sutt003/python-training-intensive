---
title: "Datasets and tips"
data: "../data_sources/*"
---

You're welcome to dive in and work as you please, but if you're feeling at a loss where to begin, follow the scaffold below.

## Project scaffold

### Task 0: Pick a dataset

We have nine datasets for you to choose from. **We recommend saving your data inside your project**.

| Dataset | Description | Source |
| --- | --- | --- |
| [World populations](../data_sources/population.csv) | A summary of world populations and corresponding statistics | Data from a [Tidy Tuesday post on 2014 CIA World Factbook data](https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-10-22/readme.md)
| [Soccer players](../data_sources/Players2024.csv) | A summary of approx. 6000 soccer players from 2024 | Data from a [Kaggle submission](https://www.kaggle.com/datasets/asemmustafa/football-players-2024-dataset).
| [Coffee survey](../data_sources/coffee_survey.csv) | A survey of blind coffee tasting results | Data from a [Kaggle submission](https://www.kaggle.com/datasets/sujaykapadnis/lets-do-some-coffee-tasting)
| [Gapminder](../data_sources/gapminder.csv) | GDP and life expectancy data by country | Data from the [Research Bazaar's R novice tutorial](https://github.com/resbaz/r-novice-gapminder), sourced from [Gapminder](https://www.gapminder.org/).
| [Melbourne housing data](../data_sources/melb_data.csv) | A collection of houses for sale in Melbourne. | Data from a [Kaggle submission](https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot) | 
| [Goodreads books](../data_sources/books.csv) | A summary of books on Goodreads. | Data from a [Kaggle submission](https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks)
| [Queensland hospitals](../data_sources/hospital_data.csv) | Queensland emergency department statistics. | Data from the Queensland Government's [Open Data Portal](https://www.data.qld.gov.au/dataset/emergency-departments-quarterly-data). |
| [Queensland fuel prices](../data_sources/qld_fuel.csv) | Fuel prices by the pump in Queensland | Data from the Queensland Government's [Open Data Portal](https://www.data.qld.gov.au/dataset/fuel-price-reporting-2024)
| [Aeroplane bird strikes](../data_sources/birds_strikes.csv) | Aeroplane bird strike incidents fron the 90s| Data from a [Tidy Tuesday post](https://github.com/rfordatascience/tidytuesday/tree/2500765853ea5235409d936cf9bbbf7d5f8b6881/data/2019/2019-07-23) sourced from an [FAA database](https://wildlife.faa.gov/home)

Remember, to load the data into Python we need to import the `pandas` module and use the `read.csv()` function.

```python
import pandas as pd
df = read.csv("path_to_data")
```

### Task 1: Understand the data

The datasets are varied with respect to variable types and content. The first exercise you should complete is a overview of the data. Use the following techniques to do so.

**Your goal: identify which variables are discrete (categorical) and which are continuous.**

#### Viewing the data structure

Use the following functions to view your data and the underlying data types.

```python
df.columns
df.info()
df.describe()
```

#### Picking out individual columns

To view the contents of particular columns, you can select them via indexing

```python
df["column_name"]
df["column_name"].unique()
df["column_name"].describe()
```

You can also apply other statistics to the column, like `.max()`.

### Task 2: Taking a subset

The datasets have lots of observations for lots of variables. To draw meaningful results, it's often useful to take a subset of those.

**Your goal: filter by a condition or group by and aggregate over a particular variable**

#### Filtering

Recall that filtering looks like indexing. If you only want to examine a certain subset of a variable, the following code will isolate that subset

```python
condition = df["column_to_filter"]...
subset = df[condition]
```

For example, if you only want "Australia" for a dataset with a "country" column, you might use 

```python
condition = df["country"] == "Australia"
```

#### Grouping

If you want to aggregate over a particular variable you need to group by it. This answers questions like, what is the average $x$ for every $y$.

If you want to group by a column and, for each of its values, apply a statistic **to all the others**,

```python
summary = df.groupby("column_to_group_by").agg("statistic")
```

If you only want to apply aggregation to some columns, we can pick them out

```python
summary = df.groupby("column_to_group_by")["column_to_aggregate"].agg("statistic")
```

> Hint: if you want to visualise the grouping variable, you might want to use `df.groupby("column", as_index = False)...` to keep it as a column

### Task 3: Visualise the relationship between variables

With your summary dataset, you can now try to visualise your variables.

**Your goal: create a visualisation of one to three variables in your summary data**.

First, you need to import the `seaborn` module

```python
import seaborn as sns
```

Next, you'll need to identify the variables to visualise. If they're both continuous, you could use a scatter or line plot

```python
sns.relplot(data = summary, x = ..., y = ..., kind = ..., ...)
```

If one of them is categorical, you could use a barplot or boxplot

```python
sns.catplot(data = summary, x = ..., y = ..., kind = ..., ...)
```

You could also consider a histogram, looking at one continuous variable

```python
sns.displot(data = summary, x = ..., ...)
```


### Task 4: Looking ahead

Now that you've performed your first analysis and visualisation of the dataset, use these results to inform your next analysis!

Below you'll find some general tips which can help. They have dataset-specific tips too, so check them out. Otherwise, feel free to ask if you have any other questions.

<!-- The t

## Getting started

Your goal is to analyse and present on one of these datasets. Getting started can sometimes be a bit daunting! For this, we've got a few initial ideas for each dataset.

```python
df.info()
```

```{python}
#| echo: false
import pandas as pd
import seaborn as sns
```

### World populations

```{python}
#| echo: false
df = pd.read_csv("../data_sources/population.csv")
```

There's a lot of columns here! We can break it down into two categories

- Identifiers, e.g. `region`, `Year`.
- Observations, e.g. `Jan.Population`, `Male.LifeExpectancy.at.Age.80..years.`.

To start small, filter the dataset for a particular region. First, use `df["Region"].unique()` to see your options. For example, to pick out Australia,

```{python}
aus = df[df["Region"] == "Australia"]
```

If you want multiple regions, you can use the **or** operator `|`: 
```{python}
aus_nz = df[(df["Region"] == "Australia") | (df["Region"] == "New Zealand")]
```

We could consider how the variables have changed over time with the `"Year"` column. We can use seaborn's `sns.relplot()` function, with `kind = "line"`, to make a line plot of the data.

To get started with this dataset,

1. Filter the data for particular region(s) 
2. Choose a variable to plot against "Year"
3. Use `sns.relplot()` to visualise the relationship

The plot below follows these steps for the `"Net.Migration.Rate..per.1.000.population."` variable.

One final tip: the variable names in this dataset are *long*. Use [`df.rename()`](#renaming-variables) to rename them.

```{python}
#| echo: false
sns.relplot(aus_nz, x = "Year", y = "Net.Migration.Rate..per.1.000.population.", hue = "Region", kind = "line")
```

### Soccer players

This dataset is being used in our workshops, so hopefully you've already got this one working!

### Coffee survey



### Gapminder

### Melbourne housing data

### Goodreads books

### Queensland hospitals

### Queensland fuel prices -->

## Tips

Here's a few general tips. In addition, we strongly recommend using [popular cheatsheets](https://rstudio.github.io/cheatsheets/), which give a quick and easy reference for common packages and functions, and [from Data to Viz](https://www.data-to-viz.com/), which guides you through choosing a visualisation.

### Hotkeys
| Code | Hotkey | Description |
| --- | --- | --- |
|  | <kbd>F9</kbd> (or <kbd>Fn</kbd> + <kbd>F9</kbd>) | Run current line |
| `# %%` | <kbd>Ctrl</kbd> + <kbd>2</kbd> | New cell (only in Spyder) |
|  | <kbd>Ctrl</kbd>+<kbd>Enter</kbd> | Run current cell (when in Script) |
|  | <kbd>Ctrl</kbd>+<kbd>C</kbd> | Cancel current operation (when in Console) |
  
### Data manipulation

Use the `pandas` package to analyse your data:

```python
import pandas as pd
```

#### Importing and exporting data
In case you've forgotten, use the `read.csv()` function to import data:

```python
df = pd.read_csv("data/dataset.csv")
```

If you'd like to export any files from Python to ".csv", use the `.to_csv()` method

```python
df.to_csv("data/output_name.csv")
```

#### Initial exploration
You'll want to explore the data to start with - below are a few functions to get started.

| Function | Example | Description |
| --- | --- | --- |
| `df.columns` | | Returns the variable names | 
| `df.info()` | | Returns the structure of the dataset (variable names, counts and types) | 
| `df["variable"]` | | Returns a specific column |
| `df["variable"].unique()` | | Returns the unique values of a variable |
| `df.describe()` or `df["variable"].describe()` | Returns a statistical summary of the dataset or a variable |

#### Removing `nan`s

We can remove `nan`s by filtering with the condition `df["variable"].notna()`:

```python
df = df[df["variable"].notna()]
```

#### Time series data
If you've picked a dataset with time-series data (e.g. a "date" variable), you should transform that variable so that it visualises better:

```python
df["variable"] = pd.to_datetime(df["variable"])
```

#### Categorical and ordered data
If you're dealing with categorical data, it can be helpful to tell Python

```python
df["variable"] = df["variable"].astype("category")
```

To manually specify the order of categories, use the `df["variable"].cat.reorder_categories()` function and use the `ordered = True` parameter

```python
df["variable"] = df["variable"].cat.reorder_categories(["cat1", "cat2", ...], ordered = True)
```

> This is particularly useful for the **Coffee survey** dataset.

If you're dealing with categorical data, look at the [pandas guide](https://pandas.pydata.org/docs/user_guide/categorical.html) for inspiration and help.

#### Renaming variables
Some datasets have cumbersome names for their variables. We can change variable names with `df.rename()`, sending a *dictionary* to the `columns = ` parameter:

```python
df = df.rename(columns = {"old_name": "new_name"})
```

This is particularly useful for the **World population** dataset.

> A *dictionary* is a Python variable with key-value pairs. The structure is `key: value`, so above we have a dictionary with one key, `"old_name"` and corresponding value `"new_name"`. They are created as follows:
> ```python
> example_dictionary = {"key1": "value1",
>                       "key2": "value2",
>                       "key3": "value3",
>                       ...}
> ```
> > Note that multiple lines are used purely for readability, you could just as well do this on one line.

### Visualisation
You can make simple visualisations with `seaborn`'s `relplot()`, `catplot()` and `displot()` functions

```python
import seaborn as sns

sns.relplot(data = df, x = "variable_x", y = "variable_y", hue = "variable_colour", ...)
```

We can add plot elements easily with `matplotlib.pyplot`
```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.relplot(data = df, x = "variable_x", y = "variable_y", hue = "variable_colour", ...)
plt.xlabel("x axis label")
plt.ylabel("y axis label")
```
